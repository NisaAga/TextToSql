def get_db_schema_description():
    """
    A helper function to get the schema for the Text-to-SQL API prompt.
    Returns the database schema as a descriptive string for the AI model.
    """

    schema_template = f"""
You are a highly skilled Text-to-SQL translator operating on a single MySQL table named `dsr_table`.
Your task is to generate precise SQL queries based on the user's natural language request.

-- CRITICAL GENERATION RULES:
-- 1. DIALECT: All queries MUST be valid **MySQL 8.0** syntax.
-- 2. QUALIFICATION: ALWAYS prefix column names with the table name (e.g., `dsr_table.station_name`).
-- 3. QUOTING: DO NOT use backticks or quotes on column names; all names are simple, clean `snake_case`.
-- 4. STRINGS: ALWAYS enclose string/text values (like names, categories..etc) in **single quotes ('')**.
-- 5. DATES: Use MySQL date functions (YEAR(), MONTH(), DATE()) on the **`report_date`** or **`date_and_time`** columns.

-- CRITICAL SEARCH PRIORITY (MAIN COLUMNS):
-- When the user asks to find data about a specific topic, event, or activity, you MUST prioritize analyzing these three columns FIRST:
-- 1. `dsr_table.call_category`
-- 2. `dsr_table.sub_category`
-- 3. `dsr_table.dsr_activity`
-- Check these columns before looking at other columns.

-- CRITICAL DATA HANDLING (REDUNDANCY & DUPLICATES):
-- When a keyword (e.g., "Storm" or any other keyword in the question) appears in multiple columns for the SAME record (row), it must NOT be double-counted.
-- You must count the distinct RECORD for the keyword, not the number of times the word appears in the row.
-- Example: If "Storm" is in `dsr_activity` AND `sub_category` for the same row, that counts as 1 incident, not 2.
-- CORRECT QUERY PATTERN: `SELECT COUNT(*) FROM dsr_table WHERE call_category LIKE '%Storm%' OR sub_category LIKE '%Storm%' OR dsr_activity LIKE '%Storm%'`. this is just an example for a particular record.
-- DO NOT use separate counts and sum them up. Use `OR` logic to capture the row once.
-- If the user asks for a 'total', 'sum', 'min', 'average', 'max', or 'count', use the appropriate aggregate function (COUNT, SUM, MIN, MAX, AVG) and return ONLY the single numeric result.

DATABASE NAME: tableqa_db
TABLE NAME: dsr_table (Daily Situation Report)

TABLE DEFINITION (Use this DDL to understand the structure):
{SCHEMA_SQL_DDL}

--- NOTE ON COLUMNS (50 Columns ‚Äì Detailed Descriptions) ---
- `report_date`: The original datetime when the report was created (DATETIME).
- `station_name`: Full name of the fire station responding to the call.
- `call_category`: High-level classification of the incident (e.g., 'Fire related', 'Emergency').
- `reinforcement_reattended`: Indicates if the call required reinforcement (e.g., 'Yes', 'No').
- `time_out`: Time (text format) when the team departed the station.
- `time_in`: Time (text format) when the team returned to base.
- `vehicle_no`: Registration number of the fire or rescue vehicle.
- `lost_human`: Number or text count of human lives lost.
- `saved_human`: Number or text count of human lives saved.
- `lost_animal`: Number or text count of animals lost.
- `saved_animal`: Number or text count of animals rescued.
- `lost_value_rs`: Estimated financial loss due to the incident in rupees (TEXT/VARCHAR).
- `saved_value_rs`: Estimated property or value saved in rupees (TEXT/VARCHAR).
- `dsr_activity`: Detailed textual activity or description of the operations performed.
- `near_location`: Landmark or general area near the site of incident.
- `at_location`: Exact address or spot where the incident occurred.
- `attended_by`: Names or ranks of personnel attending the call.
- `sub_category`: Sub-classification under main call category (e.g., 'Dry Grass & field fires').
- `taluka`: Administrative region (taluka/tehsil) where the incident occurred.
- `city_village`: Specific city or village name of the incident location.
- `additional_note_dsr`: Additional notes related to the DSR.
- `additional_remarks`: Other remarks not officially part of the DSR.
- `todays_dsr`: Summary of the DSR entry for that specific day.
- `dsr_time_text`: Textual representation of time context for the DSR (e.g., 'Morning', 'Evening').
- `lives_saved`: Text field indicating number or time context of lives saved.
- `lives_lost`: Text field representing human lives lost.
- `total_lives_lost`: Cumulative total of all lives lost across incidents or updates.
- `latitude`: Latitude coordinate of the incident.
- `longitude`: Longitude coordinate of the incident.
- `month_year`: Month and Year combined (e.g., 'Dec-2022').
- `zone`: Geographical or administrative zone (e.g., 'North Zone', '3. South Zone').
- `weekday`: Day of the week (e.g., 'Saturday').
- `hour_on_day`: Time range or specific hour (e.g., '0:00 to 2:00').
- `numerical_year`: The year stored as a four-digit integer (INT).
- `zone_and_city_village`: Merged text field combining geographical zone and the specific city/village.
- `overview_of_record`: A general summary or descriptive overview of the incident record.
- `taluka_village`: Merged field for administrative location details (Taluka and Village).
- `dsr_activity_and_note`: Merged field of the detailed DSR activity description and any additional notes.
- `near_and_at`: Merged field combining the near location landmark and the exact incident location.
- `near_at_and_by`: Merged field combining location details (near/at) and the attending personnel.
- `date_and_time`: A complete DATETIME field derived from date and time components (use this for temporal filtering).
- `filter_reinforcement`: A simplified or filtered version of the reinforcement status.
---
"""
    return schema_template.strip()










def get_db_schema_description():
    """
    A helper function to get the schema for the Text-to-SQL API prompt.
    Returns the database schema as a descriptive string for the AI model.
    """

    schema_template = f"""
You are a highly skilled deterministic Text-to-SQL translator operating on a single MySQL table named `dsr_table`.
Your task is to generate precise SQL queries based on the user's natural language request.
Each row in `dsr_table` represents exactly ONE recorded incident.


--------------------------------------------------
CORE SQL GENERATION RULES (MANDATORY)
--------------------------------------------------
1. Use MySQL 8.0 syntax only.
2. Always prefix columns with dsr_table(e.g., `dsr_table.station_name`).
3. DO NOT use backticks or quotes on column names; all names are simple, clean `snake_case`.
4. ALWAYS enclose string/text values (like names, categories..etc) in **single quotes ('')**.
5. Use COUNT(*) for incident counts.
6. Never double-count incidents.
7. Use OR conditions across columns to classify incidents.
8. Never add multiple COUNT results together.
9. Given the same question, always generate the SAME optimal SQL.
   (No randomness or alternative interpretations.)
10. If the user asks for a 'total', 'sum', 'min', 'average', 'max', or 'count', use the appropriate aggregate function (COUNT, SUM, MIN, MAX, AVG) and return ONLY the single numeric result.
11. Generate SQL that aligns with how incidents are grouped, classified, and counted in official DSR annual reports.
12. Do NOT consider 'Nil' records for any query result.
13. Do NOT consider '(-) -' records for any query result.
14. Do NOT consider '-' records for any query result.
15. Do NOT introduce additional filters beyond what is explicitly required by the question.


--Determinism Requirement:
- Given the same schema and data, identical questions MUST produce:
  - The same SQL structure
  - The same filtering logic
  - The same record counts
- When multiple SQL formulations are possible, always select the optimal one that preserves determinism.
- Do not optimize or refactor SQL for stylistic reasons.
- If the same question is executed multiple times then generate the deterministic optimal query for every execution.

-- CRITICAL SEARCH PRIORITY (MAIN COLUMNS):
-- When the user asks to find data about a specific topic, event, or activity, you MUST prioritize analyzing these three columns FIRST:
-- 1. `dsr_table.call_category`
-- 2. `dsr_table.sub_category`
-- 3. `dsr_table.dsr_activity`
-- Check these columns before looking at other columns.

--------------------------------------------------
YEAR & TIME LOGIC
--------------------------------------------------
- Use dsr_table.numerical_year for year-based filtering.
- Do NOT derive year from text fields.
- Do NOT parse year from report_date strings unless explicitly required.

Example:
  dsr_table.numerical_year = 2019

--------------------------------------------------
ZONE & CITY RELATIONSHIP
--------------------------------------------------
Cities and villages belong to administrative zones.

Zones are already pre-classified and stored in:
  dsr_table.zone

Typical zone values:
- '1. North Zone'
- '2. Central Zone'
- '3. South Zone'

Cities, villages, talukas, and stations are mapped to zones
- Make sure to check dsr.station_name first, if the area mentioned in question isn't in the station_name list then check for other
columns like: dsr.city_village, dsr.taluka, dsr.at_location, dsr.near_location, dsr.taluka_village inorder to search records for a particular given area name in the question.

- ONLY search and return records based on station_name areas mentioned in question.

IMPORTANT:
- NEVER infer zone from city, village, or station_name.
- ALWAYS filter zones using dsr_table.zone only.

--------------------------------------------------
INCIDENT CLASSIFICATION PRINCIPLES
--------------------------------------------------
Incident classification must consider:
- call_category
- sub_category
- dsr_activity
- Any other column if needed

**ONLY REFER TO THE GIVEN LISTED SUB CATEGORIES FOR EACH CALL CATEGORY DURING SEARCH OPERATION.
** Form query to appropriate nlp question asked, by first filtering call_category incidents and sub_category column incidents and finally any other column if needed like  dsr_activity, numerical_year, Zones, taluka_village...etc.**
 Below listed are the sub_category incidents under each call_category incidents.
 Do NOT consider any other incident besides listed below incidents.

-- Call Category includes the following incidents:
    1. Emergency/Accident
    2. Fire related
    3. Meteorological
    4. Hydrological
    5. Biological
    6. Geophysical
    7. Climatological
    8. Other Activities

-- Sub_category includes the following incidents:
    1. Emergency/Accident:
        - Mine flooding, open pit mine flooding
        - Chemical/Oil spills
        - Structure Collapse
        - Air, Road, Sea and Rail accidents
        - Major Liquified gas/ Chemical tanker/ receptacle incidents
        - Person trapped
        - Person rescued
        - Drowning incidents
        - Accident  in industry, storage and hazardous structure
        - Near misses
        - Other Emergency related incidents

    2. Fire related
        - Fire to &/or in a Highrise Buildings
        - Fire to &/or in a commercial/ business/ assembly/ hospital/ educational structures.
        - Fire to &/or in a residential low rise structures, flat, house, village.
        - Fire to &/or in a slum area, huts, labour camp.
        - Fire to temporary structures.
        - Fires to &/or in Industries, Storage & Hazardous structures.
        - Dry Grass & field fires.
        - Wildland fires.
        - Electrical related fires.
        - Inflammable/toxic chemical & liquefied gas incidents.
        - Air, Road, Sea and Rail fire incidents.
        - Arson
        - Garbage and Scrap Fire.
        - False alarms/ Unconfirmed.
        - Other Fire related incidents.

    3. Meteorological
        - Cyclone, Storm Surge, Tornado, Convective Storm, Extratropical Storm, Wind.
        - Cloud Burst
        - Cold Wave, Derecho.
        - Extreme Temperature, Fog, Frost, Freeze, Hail.
        - Lightning, Heavy Rain and Wind
        - Sand-Storm, Dust-Storm
        - Heat-wave

    4. Hydrological
        - Coastal Erosion
        - Coastal flood
        - Flash Flood Hydrological
        - Flood Hydrological
        - Drainage Management

    5. Biological
        - Epidemics
        - Insect infestations
        - Animal stampedes
        - Food poisoning

    6. Geophysical
        - Landslides and mudflows
        - Earthquakes
        - Tsunami
        - Dam failures/Dam Bursts

    7. Climatological
        - Drought
        - Extreme hot/cold conditions
        - Forest/Wildfire Fires
        - Subsidence

    8. Other Activities
        - Mock Drills
        - Special Service Calls
        - Other Activities

** Example 1: user asks question such as "Total number of drowned people in panaji in the year 2019", you should filter the call category by emergency/accident related incidents,
 then filter zones column by 'North' and filter at_location column by 'Panaji', then filter the sub_category by 'Drowning incidents', filter numerical_year by '2019',
 and then filter dsr_activity column and count the total number of people drowned using aggregate function count(). and return the final count.**

** Example 2: user asks question such as "Total number of meteorological incidents in the year 2024", you should filter the call category by meteorological,
 then filter numerical_year column by '2024', consider all the sub_category incidents and count all records using aggregate function count().**

** Example 3: user asks question such as "Number of animals rescued in the year 2017", you should filter the call category by Emergency/accident, then filter numerical_year column by '2017'
and finally count the total number of animals rescued by the 'saved_animals' column using aggregate function count() and return the final count.**

** When a question is asked which contains the keywords 'all', 'details','show',... etc, DO NOT return all columns from the database, infact return the columns that are required based on the question asked**
** Example 4: user asks question such as "Details of car accidents", you should filter the dsr_activity column and search for the keywords 'car accident' or 'vehicle accident' or 'accident caused by car/vehicle/truck...etc',
 and select the most relevant columns needed to show the details for each record of car accident, return all the filtered rows with required columns. Do NOT return unnecessary columns such as 'animals_saved', 'longitude', latitude'...etc **

** Example 5: user asks question such as "Show incidents with long response durations.", you should first calculate the response time by referring to dsr_time_text column for all records, then filter all records based on maximum response time,
 and finally return all incidents based on maximum response time.**

Make sure to return results only based on the question asked. Do not include or count redundant records.
Do NOT rely on a single column.

--------------------------------------------------
Call Duration Semantics (MANDATORY)
--------------------------------------------------

A call is considered closed when both time_in and time_out are present.
time_in represents the call start time.
time_out represents the call end time.
Both columns must be interpreted as TIME values on the same report_date.

Duration must be calculated as:
TIMESTAMPDIFF(
  MINUTE,
  TIMESTAMP(report_date, time_in),
  TIMESTAMP(report_date, time_out)
)

Rows must be excluded if:
time_in IS NULL
time_out IS NULL
time_in = '' (empty string)
time_out = '' (empty string)

Calls are categorized as:
within_30_minutes ‚Üí duration ‚â§ 30 minutes
over_30_minutes ‚Üí duration > 30 minutes
Do not use alternative parsing functions (STR_TO_DATE, CONCAT, CAST) unless explicitly required.
Do not infer alternative time columns.
--------------------------------------------------
COUNTING RULES (CRITICAL)
--------------------------------------------------
- Each row = ONE incident.
- If an incident matches multiple conditions,
  it must still be counted only ONCE.
- Use a single COUNT(*) with OR conditions.
- Never count the same row multiple times.

--------------------------------------------------
DETERMINISTIC BEHAVIOR (MANDATORY)
--------------------------------------------------
You MUST behave deterministically.

For the same question:
- Always generate the same SQL.
- Do not change column choice, filters, or logic.

--------------------------------------------------
TABLE STRUCTURE
--------------------------------------------------
NAME: tableqa_db
TABLE NAME: dsr_table (Daily Situation Report)

TABLE DEFINITION (Use this DDL to understand the structure):
{SCHEMA_SQL_DDL}

--- NOTE ON COLUMNS (50 Columns ‚Äì Detailed Descriptions) ---
- `report_date`: The original datetime when the report was created (DATETIME).
- `station_name`: Full name of the fire station responding to the call.
- `call_category`: High-level classification of the incident (e.g., 'Fire related', 'Emergency').
- `reinforcement_reattended`: Indicates if the call required reinforcement (e.g., 'Yes', 'No').
- `time_out`: Time (text format) when the team departed the station.
- `time_in`: Time (text format) when the team returned to base.
- `vehicle_no`: Registration number of the fire or rescue vehicle.
- `lost_human`: Number or text count of human lives lost.
- `saved_human`: Number or text count of human lives saved.
- `lost_animal`: Number or text count of animals lost.
- `saved_animal`: Number or text count of animals rescued.
- `lost_value_rs`: Estimated financial loss due to the incident in rupees (TEXT/VARCHAR).
- `saved_value_rs`: Estimated property or value saved in rupees (TEXT/VARCHAR).
- `dsr_activity`: Detailed textual activity or description of the operations performed.
- `near_location`: Landmark or general area near the site of incident.
- `at_location`: Exact address or spot where the incident occurred.
- `attended_by`: Names or ranks of personnel attending the call.
- `sub_category`: Sub-classification under main call category (e.g., 'Dry Grass & field fires').
- `taluka`: Administrative region (taluka/tehsil) where the incident occurred.
- `city_village`: Specific city or village name of the incident location.
- `additional_note_dsr`: Additional notes related to the DSR.
- `additional_remarks`: Other remarks not officially part of the DSR.
- `todays_dsr`: Summary of the DSR entry for that specific day.
- `dsr_time_text`: Textual representation of time context for the DSR (e.g., 'Morning', 'Evening').
- `lives_saved`: Text field indicating number or time context of lives saved.
- `lives_lost`: Text field representing human lives lost.
- `total_lives_lost`: Cumulative total of all lives lost across incidents or updates.
- `latitude`: Latitude coordinate of the incident.
- `longitude`: Longitude coordinate of the incident.
- `month_year`: Month and Year combined (e.g., 'Dec-2022').
- `zone`: Geographical or administrative zone (e.g., 'North Zone', '3. South Zone').
- `weekday`: Day of the week (e.g., 'Saturday').
- `hour_on_day`: Time range or specific hour (e.g., '0:00 to 2:00').
- `numerical_year`: The year stored as a four-digit integer (INT).
- `zone_and_city_village`: Merged text field combining geographical zone and the specific city/village.
- `overview_of_record`: A general summary or descriptive overview of the incident record.
- `taluka_village`: Merged field for administrative location details (Taluka and Village).
- `dsr_activity_and_note`: Merged field of the detailed DSR activity description and any additional notes.
- `near_and_at`: Merged field combining the near location landmark and the exact incident location.
- `near_at_and_by`: Merged field combining location details (near/at) and the attending personnel.
- `date_and_time`: A complete DATETIME field derived from date and time components (use this for temporal filtering).
- `filter_reinforcement`: A simplified or filtered version of the reinforcement status.
---
"""
    return schema_template.strip()




 # IMPORT RULES

column names should be:
    ['report_date', 'station_name',
    'call_category', 'reinforcement_reattended', 'time_out', 'time_in',
    'vehicle_no', 'lost_human', 'saved_human', 'lost_animal', 'saved_animal',
    'lost_value_rs', 'saved_value_rs', 'dsr_activity', 'near_location',
    'at_location', 'attended_by', 'sub_category', 'taluka', 'city_village',
    'lives_saved', 'lives_lost', 'total_lives_lost',
    'month_year', 'zone', 'weekday', 'numerical_year',
    'taluka_village',
    'date_and_time']

datatypes for each column:
    `report_date`:TIME,
    `station_name`:VARCHAR(255),
    `call_category`:VARCHAR(255),
    `reinforcement_reattended`:VARCHAR(50),
    `time_out`:VARCHAR(50),
    `time_in`:VARCHAR(50),
    `vehicle_no`:VARCHAR(50),
    `lost_human`:INT(50),
    `saved_human`:INT(50),
    `lost_animal`:INT(50),
    `saved_animal`:INT(50)
    `lost_value_rs`:INT(50),
    `saved_value_rs`:INT(50),
    `dsr_activity`:TEXT,
    `near_location`:TEXT,
    `at_location`:TEXT,
    `attended_by`:TEXT,
    `sub_category`:TEXT,
    `taluka`:TEXT,
    `city_village`:TEXT,
    `lives_saved`:VARCHAR(50),
    `lives_lost`:VARCHAR(50),
    `total_lives_lost`:INT,
    `month_year`: VARCHAR(255),
    `zone`: VARCHAR(255),
    `weekday`:VARCHAR(255),
    `numerical_year`:YEAR,
    `taluka_village`:VARCHAR(255),
    `date_and_time`:VARCHAR(255),

# data cleaning rules of dsr excel sheet data of all years
1) NIL entries of rows should be deleted
2) NULL entries of rows should be deleted

3) For call_category column:
    -NIL,NULL call_categories to be deleted
    -Call_categories: 'Specical service', 'mock drill' have to be in sub_category column instead of call_category.
    -'Special service', 'mock drill' sub categories of call category 'Other activities'.
    -There are 2 spellings for 'reinforcement' (update records from call category where ‚Äòreinforcement-‚Äò to ‚Äòreinforcement‚Äô )

4) For the station name column
    - There are 2 spellings for 'Curchorem' station. 1 record has spelling 'Curchorm' (Change spelling of station name ‚ÄòCurchorm‚Äô to ‚ÄòCurchorem‚Äô)
    - NULL station name should be deleted.

5) For the sub_category column
    -merge records of ‚ÄòDrowning incidents‚Äô to ‚ÄúDrowning, suicide and other related incidents‚Äô and remove 'Drowning incidents' entry in sub_category column. But DO NOT delete any record from this categories.
    -Change spelling of records from ‚ÄòFire to &/or in a commercial/ bussiness/ assembly/ hospital/ educational structures‚Äô into ‚ÄòFire to &/or in a commercial/ business/ assembly/ hospital/ educational structures‚Äô from sub_category column. But DO NOT delete any record from this categories.
    -Change spelling of records from ‚ÄòFire to &/or in a residential low rise structures, house, village‚Äô into ‚ÄòFire to &/or in a residential low rise structures, flat, house, village‚Äô from sub_category column. But DO NOT delete any record from this categories.

6) For saved_human column
    -All saved_column entries should be integers only indicating the count of saved human lives.
    -There is one entry with 'R11'. Do not update it. keep it like that.

7) For saved_animal column
    -There is one entry with 'R2B'. Do not update it. keep it like that.
    -There is one entry with '01 L' update it to value '1'.










import os
import glob
import pandas as pd
import mysql.connector
import numpy as np

# -------------------------------
# CONFIGURATION
# -------------------------------
DATA_DIR = "new_data/"
MYSQL_CONFIG = {
    "host": "localhost",
    "user": "root",
    "password": "",
    "database": "dsr"
}

TABLE_NAME = "dsr_table"

COLUMN_ORDER = [
    'report_date', 'station_name', 'call_category', 'reinforcement_reattended',
    'time_out', 'time_in', 'vehicle_no', 'lost_human', 'saved_human',
    'lost_animal', 'saved_animal', 'lost_value_rs', 'saved_value_rs',
    'dsr_activity', 'near_location', 'at_location', 'attended_by',
    'sub_category', 'taluka', 'city_village', 'lives_saved',
    'lives_lost', 'total_lives_lost', 'zone',
    'weekday', 'numerical_year', 'taluka_village', 'date_and_time'
]

# -------------------------------
# DATABASE CONNECTION
# -------------------------------
def get_db():
    return mysql.connector.connect(**MYSQL_CONFIG)

# -------------------------------
# DATA CLEANING FUNCTION
# -------------------------------
def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:

    # Normalize column names
    df.columns = (
        df.columns.str.strip()
        .str.lower()
        .str.replace(" ", "_")
        .str.replace("/", "_")
    )

    # Keep only required columns
    df = df[[c for c in COLUMN_ORDER if c in df.columns]]

    # -------------------------------
    # 1 & 2) Remove NIL / NULL rows
    # -------------------------------
    df.replace(["NIL", "nil", ""], np.nan, inplace=True)
    df.dropna(how="all", inplace=True)
    df.dropna(inplace=True)

    # -------------------------------
    # 3) call_category rules
    # -------------------------------
    df = df[df["call_category"].notna()]

    df["call_category"] = df["call_category"].str.strip()

    # reinforcement spelling fix
    df["call_category"] = df["call_category"].str.replace(
        "reinforcement-", "reinforcement", regex=False
    )

    # # move Special service / mock drill
    # mask_special = df["call_category"].str.lower().isin(
    #     ["special service", "special service", "mock drill"]
    # )
    #
    # df.loc[mask_special, "sub_category"] = df.loc[mask_special, "call_category"]
    # df.loc[mask_special, "call_category"] = "Other activities"

    # -------------------------------
    # 4) station_name rules
    # -------------------------------
    df = df[df["station_name"].notna()]
    df["station_name"] = df["station_name"].replace("Curchorm", "Curchorem")

    # -------------------------------
    # 5) sub_category rules
    # -------------------------------
    df["sub_category"] = df["sub_category"].replace(
        "Drowning incidents",
        "Drowning, suicide and other related incidents"
    )

    df["sub_category"] = df["sub_category"].replace(
        "Fire to &/or in a commercial/ bussiness/ assembly/ hospital/ educational structures",
        "Fire to &/or in a commercial/ business/ assembly/ hospital/ educational structures"
    )

    df["sub_category"] = df["sub_category"].replace(
        "Fire to &/or in a residential low rise structures, house, village",
        "Fire to &/or in a residential low rise structures, flat, house, village"
    )

    # -------------------------------
    # 6) saved_human
    # -------------------------------
    def clean_saved_human(x):
        if x == "R11":
            return x
        try:
            return int(x)
        except:
            return None

    df["saved_human"] = df["saved_human"].apply(clean_saved_human)

    # -------------------------------
    # 7) saved_animal
    # -------------------------------
    def clean_saved_animal(x):
        if x == "R2B":
            return x
        if x == "01 L":
            return "1"
        try:
            return int(x)
        except:
            return None

    df["saved_animal"] = df["saved_animal"].apply(clean_saved_animal)

    # Numeric coercion
    for col in ["lost_human", "lost_animal", "lost_value_rs",
                "saved_value_rs", "total_lives_lost"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    return df

# -------------------------------
# INSERT INTO MYSQL
# -------------------------------
def insert_to_mysql(df: pd.DataFrame):
    conn = get_db()
    cursor = conn.cursor()

    placeholders = ", ".join(["%s"] * len(COLUMN_ORDER))
    columns = ", ".join(COLUMN_ORDER)

    sql = f"INSERT INTO {TABLE_NAME} ({columns}) VALUES ({placeholders})"

    cursor.executemany(sql, df[COLUMN_ORDER].values.tolist())
    conn.commit()

    cursor.close()
    conn.close()

# -------------------------------
# MAIN PROCESS
# -------------------------------
def run_import():
    files = glob.glob(os.path.join(DATA_DIR, "*.xlsx"))

    print(f"üìÅ Found {len(files)} Excel files")

    for file in files:
        print(f"‚û° Processing {os.path.basename(file)}")
        df = pd.read_excel(file)
        df_cleaned = clean_dataframe(df)
        insert_to_mysql(df_cleaned)
        print(f"‚úî Inserted {len(df_cleaned)} rows")

    print("üéâ All files imported successfully")

# -------------------------------
if __name__ == "__main__":
    run_import()





app.py backup 4th feb 26
from flask import Flask, request, jsonify, render_template
from database.mysql_connector import MySQLExecutor, get_db_schema_description
from service.sqlai_api import SQLAIAPI
from config import CURRENT_TEXT2SQL_PROVIDER
from datetime import timedelta   # <-- ADDED
from openpyxl import Workbook
from flask import send_file
from tempfile import NamedTemporaryFile


# --- AGGREGATE INTENT (NL FALLBACK ONLY) ---
def is_aggregate_query_from_nl(nl_query: str) -> bool:
    """
    Weak signal: checks if NL query *suggests* aggregation.
    Used only as a fallback.
    """
    nl = nl_query.lower()

    aggregate_keywords = [
        "total",
        "number of",
        "how many",
        "count",
        "sum",
        "average",
        "avg",
        "maximum",
        "minimum",
        "max",
        "min"
    ]

    return any(keyword in nl for keyword in aggregate_keywords)


# --- AGGREGATE INTENT (AUTHORITATIVE: SQL-BASED) ---
def is_aggregate_query_from_sql(sql: str) -> bool:
    """
    Strong signal: checks actual SQL for aggregate functions.
    """
    if not sql:
        return False

    sql_lower = sql.lower()
    aggregate_functions = ["count(", "sum(", "avg(", "min(", "max("]

    return any(fn in sql_lower for fn in aggregate_functions)


# --- FALLBACK STRATEGY CLASS ---
class APIKeyMissingStrategy:
    def __init__(self):
        print("WARNING: API Key Missing Strategy active.")

    def execute_text_to_sql(self, natural_language_query: str, db_schema: str) -> str:
        return "ERROR: AI2SQL API key not found or invalid. Please check config.py."


app = Flask(__name__)


# --- JSON SAFETY (timedelta only) ---
def _json_safe(value):
    if isinstance(value, timedelta):
        return value.total_seconds()
    return value

def generate_excel(headers, rows, sheet_name="Report"):
    wb = Workbook(write_only=True)
    ws = wb.create_sheet(title=sheet_name)

    # Write header row
    ws.append(headers)

    # Write data rows
    for row in rows:
        ws.append(row)

    temp_file = NamedTemporaryFile(delete=False, suffix=".xlsx")
    wb.save(temp_file.name)
    wb.close()

    return temp_file.name



# --- AGENT ORCHESTRATION CLASS ---
class Text2SQLAgent:
    def __init__(self, sql_generator_strategy):
        self.generator = sql_generator_strategy
        self.executor = MySQLExecutor()
        self.db_schema = get_db_schema_description()

    def process_query(self, nl_query: str):
        if not self.executor.db_is_ready:
            return "DB_ERROR", None, "DB not ready. Check startup logs."

        # Phase 1: Generate SQL
        generated_sql = self.generator.execute_text_to_sql(
            nl_query,
            self.db_schema
        )

        if generated_sql.startswith("ERROR"):
            return "API_ERROR", generated_sql, None

        # Phase 2: Execute SQL
        try:
            with self.executor as db:
                headers, results = db.execute(generated_sql)

            return "SUCCESS", generated_sql, {
                "headers": headers,
                "results": results
            }

        except Exception as e:
            return "DB_ERROR", generated_sql, f"{type(e).__name__}: {str(e)}"


# --- STRATEGY INITIALIZATION ---
try:
    text2sql_strategy = SQLAIAPI()
    active_provider_name = "AI2SQL (Active)"
    print("AI2SQL API initialized successfully.")
except (ValueError, ImportError) as e:
    text2sql_strategy = APIKeyMissingStrategy()
    active_provider_name = "AI2SQL (Key Missing)"
    print(f"Fallback mode activated: {e}")

text2sql_agent = Text2SQLAgent(text2sql_strategy)
print(f"Active Provider: {active_provider_name}")


# --- ROUTES ---
@app.route('/')
def index():
    return render_template("index.html", provider_name=active_provider_name)


@app.route('/api/provider_name', methods=['GET'])
def get_provider_name():
    return jsonify({"provider": active_provider_name})


@app.route('/api/query', methods=['POST'])
def handle_query():
    data = request.json
    natural_language_query = data.get("query")

    if not natural_language_query:
        return jsonify({"error": "No query provided"}), 400

    status, generated_sql, result_data = text2sql_agent.process_query(
        natural_language_query
    )

    # --- API ERROR ---
    if status == "API_ERROR":
        return jsonify({
            "show_generated_sql": False,
            "generated_sql": None,
            "headers": ["AI Generation Error"],
            "results": [[generated_sql]],
            "provider": active_provider_name
        })

    # --- AGGREGATE DECISION (FINAL) ---
    aggregate_intent = (
        is_aggregate_query_from_sql(generated_sql)
        or is_aggregate_query_from_nl(natural_language_query)
    )

    # --- DB ERROR ---
    if status == "DB_ERROR":
        return jsonify({
            "show_generated_sql": aggregate_intent,
            "generated_sql": generated_sql if aggregate_intent else None,
            "headers": ["DB Execution Error"],
            "results": [[result_data]],
            "provider": active_provider_name
        })

    # --- SUCCESS ---
    return jsonify({
        "show_generated_sql": aggregate_intent,
        "generated_sql": generated_sql if aggregate_intent else None,
        "headers": result_data["headers"],
        "results": [
            [_json_safe(v) for v in row]
            for row in result_data["results"]
        ],
        "provider": active_provider_name
    })


@app.route('/api/test_db_records', methods=['GET'])
def test_db_records():
    test_query = "SELECT * FROM dsr_table LIMIT 50;"

    if not text2sql_agent.executor.db_is_ready:
        return jsonify({
            "show_generated_sql": True,
            "generated_sql": test_query,
            "headers": ["DB Connection Error"],
            "results": [["DB not ready"]],
            "provider": active_provider_name
        }), 500

    try:
        with text2sql_agent.executor as db:
            headers, results = db.execute(test_query)

        return jsonify({
            "show_generated_sql": True,
            "generated_sql": test_query,
            "headers": headers,
            "results": [
                [_json_safe(v) for v in row]
                for row in results
            ],
            "provider": active_provider_name
        })

    except Exception as e:
        return jsonify({
            "show_generated_sql": True,
            "generated_sql": test_query,
            "headers": ["DB Execution Error"],
            "results": [[str(e)]],
            "provider": active_provider_name
        }), 500

@app.route('/api/query/export/excel', methods=['POST'])
def export_query_to_excel():
    data = request.json
    natural_language_query = data.get("query")

    if not natural_language_query:
        return jsonify({"error": "No query provided"}), 400

    status, generated_sql, result_data = text2sql_agent.process_query(
        natural_language_query
    )

    if status != "SUCCESS":
        return jsonify({
            "error": "Query execution failed",
            "details": result_data
        }), 500

    headers = result_data["headers"]
    rows = result_data["results"]

    file_path = generate_excel(
        headers=headers,
        rows=rows,
        sheet_name="AI_Report"
    )

    return send_file(
        file_path,
        as_attachment=True,
        download_name="ai_generated_report.xlsx",
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


if __name__ == "__main__":
    app.run(debug=True)


